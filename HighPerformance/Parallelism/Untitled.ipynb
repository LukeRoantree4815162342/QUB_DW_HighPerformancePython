{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Parallelism?\n",
    "\n",
    "## Why would that be useful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context:\n",
    "\n",
    "So to show the advantage of parallelism, and make it more clear in our examples what the differences between parallel and serial setups are, we're going to build a couple of monte-carlo integrators - that is numerical approximations of definite integrals via monte-carlo sampling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo integration approximates the area under a curve by averaging the heights of many random samples from the curve;\n",
    "![example](test_func_base.svg)\n",
    "The more points that are taken, the better the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial solution (serial)\n",
    "\n",
    "lets say we want to integrate some function, $f(x)=\\text{tanh}\\left(x^{\\frac{1}{3}} + x^{-3} - \\frac{3}{2}x^{-1}\\right)$, between $x=0.1$ and $x=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arbitrary_func(x):\n",
    "    return np.tanh(x**(1/3) + x**(-3) - (3/2)*x**-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "monte carlo solutions -\n",
      "10 samples:    0.8920320635525698\n",
      "100 samples:   1.2680625801108831\n",
      "1000 samples:  1.2397725482226427\n",
      "10000 samples: 1.2333568520980813\n",
      "\n",
      "scipy's fancy integrator says the answer is about 1.24169 +- 8.91e-09\n"
     ]
    }
   ],
   "source": [
    "def get_sample(func, a, b):\n",
    "    random_pos = np.random.uniform(a,b)\n",
    "    return func(random_pos)\n",
    "\n",
    "def monte_carlo_integrate_serial(func, a, b, sample_size=1000):\n",
    "    samples = [get_sample(func, a, b) for _ in range(sample_size)]\n",
    "    return (b-a) * sum(samples)/sample_size\n",
    "\n",
    "print(\"\"\"\n",
    "monte carlo solutions -\n",
    "10 samples:    {}\n",
    "100 samples:   {}\n",
    "1000 samples:  {}\n",
    "10000 samples: {}\n",
    "\"\"\".format(monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 10),\n",
    "           monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 100),\n",
    "           monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 1000),\n",
    "           monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 10000)\n",
    "          )\n",
    "     )\n",
    "\n",
    "print(\"scipy's fancy integrator says the answer is about {0[0]:.6g} +- {0[1]:.3g}\".format(\n",
    "    integrate.quadrature(arbitrary_func, 0.1, 2))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obviously this isn't optimised anywhere near as well as it could have been - not includung that basic monte carlo integration isn't fantastic in general, we could have used numpy vectorisation to generate all the random samples with a single call to get_sample. Ignoring that, because that isn't the focus of this section, we're going to try to improve it by parallelising it.\n",
    "\n",
    "#### First, let's say we want to get an accuracy of about 10^-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "monte carlo solutions -\n",
      "error @ 10 samples:       -0.2492\n",
      "error @ 100 samples:      0.0669\n",
      "error @ 1000 samples:     0.006989\n",
      "error @ 10000 samples:    0.004879\n",
      "error @ 100000 samples:   0.0008561\n",
      "error @ 1000000 samples:  -0.0001551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scipy_ans = integrate.quadrature(arbitrary_func, 0.1, 2)[0]\n",
    "print(\"\"\"\n",
    "monte carlo solutions -\n",
    "error @ 10 samples:       {:.4g}\n",
    "error @ 100 samples:      {:.4g}\n",
    "error @ 1000 samples:     {:.4g}\n",
    "error @ 10000 samples:    {:.4g}\n",
    "error @ 100000 samples:   {:.4g}\n",
    "error @ 1000000 samples:  {:.4g}\n",
    "\"\"\".format(monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 10)-scipy_ans,\n",
    "           monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 100)-scipy_ans,\n",
    "           monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 1000)-scipy_ans,\n",
    "           monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 10000)-scipy_ans,\n",
    "           monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 100000)-scipy_ans,\n",
    "           monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 1000000)-scipy_ans,\n",
    "          )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So it looks like we need to take about 2,000,000 samples to get that level of accuracy. But each sample is completely independent from the others - so can we take more than one sample at a time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1457672119140625e-05\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "serial_out = monte_carlo_integrate_serial(arbitrary_func, 0.1, 2, 2000000)\n",
    "end = time()\n",
    "print(end-start) # baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2400916018035182 34.43357992172241\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "\"\"\"\n",
    "Method one:\n",
    "\"\"\"\n",
    "sample_size = 2000000\n",
    "# We need to define a function that generates one sample - we'll adapt that from our serial one\n",
    "def par_func(x):\n",
    "    return monte_carlo_integrate_serial(arbitrary_func,0.1,2,1)\n",
    "\n",
    "p = Pool(processes=cpu_count())\n",
    "\n",
    "start = time()\n",
    "par_out = p.map(par_func, range(sample_size))\n",
    "end = time()\n",
    "\n",
    "print(np.mean(par_out), end-start)\n",
    "\n",
    "# Takes ages - we have a lot of work starting new processes, and moving data between the cpu cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2420546588264603 8.815419673919678\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "\"\"\"\n",
    "Method two:\n",
    "\"\"\"\n",
    "sample_size = 2000000\n",
    "batch_size = 100000\n",
    "# We need to define a function that generates one sample - we'll adapt that from our serial one\n",
    "def par_func2(x):\n",
    "    return monte_carlo_integrate_serial(arbitrary_func,0.1,2,batch_size)\n",
    "\n",
    "p = Pool(processes=cpu_count())\n",
    "\n",
    "start = time()\n",
    "par_out = p.map(par_func2, range(sample_size//batch_size))\n",
    "end = time()\n",
    "\n",
    "print(np.mean(par_out), end-start)\n",
    "# Here we get the same thing, but we've reduced the number of times we need to start new processes or move data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2414749491833201 8.1034414768219\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\"\"\"\n",
    "Method three:\n",
    "\"\"\"\n",
    "start = time()\n",
    "par_out = Parallel(n_jobs=-1)(\n",
    "    delayed(\n",
    "            par_func2\n",
    "           )(_) \n",
    "           for _ in range(sample_size//batch_size))\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(np.mean(par_out), end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMethod four: mpi4py\\n(see other notebook in this directory)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Method four: mpi4py\n",
    "(see other notebook in this directory)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
